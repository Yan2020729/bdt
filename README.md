### bdt
The R package **bdt** implements adapted bootstrap diagnostic tool (BDT), originally described in Bahamyirou A, Blais L, Forget A, Schnitzer ME. *Understanding and diagnosing the potential for bias when using machine learning methods with doubly robust causal estimators*. \emph{Statistical Methods in Medical Research}. 2019 Jun;28(6):1637-50. This **bdt** package is to identify the instability in the estimation of marginal causal effects in the presence of near practical positivity violations. Data-adaptive methods can produce a separation of the two exposure groups in terms of propensity score densities which can lead to biased finite-sample estimates of the treatment effect. **bdt** is based on a bootstrap resampling of the subjects and simulation of the outcome data conditional on observed treatment and covariates in order to diagnose whether the estimation using data-adaptive methods for the propensity score may lead to large bias and poor coverage in the finite sample. For more theoretical details, please refer to the original paper: https://doi.org/10.1177%2F0962280218772065.

In the context of causal inference, identifiability of causal effects requires some untestable assumptions that allow investigators to write the parameter of interest in terms of the distribution of the observational studies. The common assumptions include stable unit treatment value assumption, consistency, ignorability and positivity. Consider an observational data with n i.i.d. observations on the data structure O = (W, A, Y), where W be the vector of baseline covariates of an individual; A be the observed binary treatment indicator with the value equals to 1 if the subject received the treatment and 0 otherwise; Y be the observed continuous or binary outcome. The parameter of interest is the additive effect conditioning on a set of potential confounders W. Positivity assumption requires that the probability of receiving any level of treatment conditioning on every set of covariates must be positivity for all subjects in the population, i.e. P(A=a|W)>0 for any a=0 or 1. 

In this package, to estimate the average treatment effect (ATE), we use the inverse probability of treatment weighting (IPTW), augmented inverse probability of treatment weighted estimator (A-IPTW) and Targeted Maximum Likelihood Estimators (TMLEs). To estimate the unbiased or consistent estimator, the correct specification of treatment assignment model is required for IPTW. Doubly robust estimators A-IPTW and TMLE decrease the dependence on the correct specification of the propensity model and only require either outcome model or propensity score model to be correctly specified. 

In practice, researchers are typically unaware of the true specification of the propensity score model. To increase the chance of correct model specification, flexible prediction methods, in particular the ensemble learner called 'SuperLearner' (SL), are often recommended. SL is a nonparametric methodology that uses cross validation to find an optimal convex combination of the predictions of a library of candidate algorithms defined by the user. However, the flexibility in the propensity modeling can lead to the selection of strong predictors of the treatment. This would give rise to extreme value of propensity score since these predictors may or may not be real confounders. Both A-IPTW and TMLE involve the inverse of propensity scores and practical positivity violations can lead to unstable parameter estimates and potential bias due to highly variable weights. In order to address this issue, one may define different bounds to truncate the probabilities then reduce their standard errors though choice of truncation level is usually ad-hoc.

Furthermore, in the presence of potential positivity violations, flexible modeling of the propensity score give rise to the increment of the bias and variance of the causal effect estimators as compared to parametric methods and the extent of how much they are impacted depends on the estimator. Bahamyirou et al. presented an adapted version of the diagnostic tool proposed by Peterson et al. in simulated data to illustrate whether the ATE estimators were destablized by the use of SL to fit the propensity score. For a given observed data set, the diagnostic tool which is based on a bootstrap resampling of the subjects, keeps the observed treatment of each resampled subject but simulates the outcome using the information of the two observed subgroups who are treated or not being treated. To expedite the application of this diagnostic tool, we develop **bdt** package to diagnose the instability introduced when machine learning is employed for the estimation of the propensity score. 


